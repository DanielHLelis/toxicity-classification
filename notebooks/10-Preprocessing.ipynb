{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '../'\n",
    "DRIVE_PATH = 'Colab/ToxicityClassification'\n",
    "\n",
    "# When on Colab, use Google Drive as the root path to persist and load data\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    ROOT_PATH = os.path.join('/content/drive/My Drive/', DRIVE_PATH)\n",
    "    os.makedirs(ROOT_PATH, exist_ok=True)\n",
    "    os.chdir(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(os.path.join(ROOT_PATH, 'data', 'joint', 'data.parquet.zstd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Toolkits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "spacy.cli.download('pt_core_news_sm')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49098eb7640e4efeb6b0ae21729f7344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27952 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleanup_re = re.compile(r'[\\W\\s]')\n",
    "remove_double_spaces_re = re.compile(r'\\s+')\n",
    "\n",
    "base_clean = []\n",
    "base_clean_lower = []\n",
    "tokenized = []\n",
    "lemmatized = []\n",
    "no_accents = []\n",
    "lemma_no_accents = []\n",
    "no_stop_words = []\n",
    "lemma_no_stop_words = []\n",
    "no_stop_words_no_accents = []\n",
    "lemma_no_stop_words_no_accents = []\n",
    "\n",
    "\n",
    "# TODO: generalize each pre-processing approach into a separate function\n",
    "# in a separate file.\n",
    "for row in tqdm(df.iter_rows(named=True), total=len(df)):\n",
    "    text: str = row['text']\n",
    "    # Remove bad characters\n",
    "    text = cleanup_re.sub(' ', text)\n",
    "    text = remove_double_spaces_re.sub(' ', text)\n",
    "    text = text.strip()\n",
    "    base_clean.append(text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    base_clean_lower.append(text)\n",
    "\n",
    "    # Tokenize\n",
    "    # TODO: go deeper into tokenization\n",
    "    tokens = [token for token in nlp(text)]\n",
    "    tokenized.append([token.text for token in tokens])\n",
    "\n",
    "    # Lemmatized\n",
    "    lemmatized.append([token.lemma_ for token in tokens])\n",
    "\n",
    "    # No accents\n",
    "    no_accents.append([unidecode.unidecode(token.text) for token in tokens])\n",
    "    lemma_no_accents.append([unidecode.unidecode(token.lemma_) for token in tokens])\n",
    "\n",
    "    # No stop words\n",
    "    no_stop_words.append([token.text for token in tokens if not token.is_stop])\n",
    "    lemma_no_stop_words.append([token.lemma_ for token in tokens if not token.is_stop])\n",
    "\n",
    "    # No stop words, no accents\n",
    "    no_stop_words_no_accents.append([unidecode.unidecode(token.text) for token in tokens if not token.is_stop])\n",
    "    lemma_no_stop_words_no_accents.append([unidecode.unidecode(token.lemma_) for token in tokens if not token.is_stop])\n",
    "\n",
    "\n",
    "\n",
    "df_ext = df.with_columns([\n",
    "    pl.Series('base_clean', base_clean),\n",
    "    pl.Series('base_clean_lower', base_clean_lower),\n",
    "    pl.Series('tokenized', tokenized),\n",
    "    pl.Series('lemmatized', lemmatized),\n",
    "    pl.Series('no_accents', no_accents),\n",
    "    pl.Series('lemma_no_accents', lemma_no_accents),\n",
    "    pl.Series('no_stop_words', no_stop_words),\n",
    "    pl.Series('lemma_no_stop_words', lemma_no_stop_words),\n",
    "    pl.Series('no_stop_words_no_accents', no_stop_words_no_accents),\n",
    "    pl.Series('lemma_no_stop_words_no_accents', lemma_no_stop_words_no_accents)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext.write_parquet(os.path.join(ROOT_PATH, \"data/joint/pre_processed_data.parquet.zstd\"), compression=\"zstd\", compression_level=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
